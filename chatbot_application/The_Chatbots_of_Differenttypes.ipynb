{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66111f7-274f-49dd-9eab-6f99607057ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import configs, build_model\n",
    "\n",
    "# Load pre-trained sentiment analysis model\n",
    "model_config = configs.classifiers.rusentiment_bert\n",
    "\n",
    "# Load the model\n",
    "model = build_model(model_config, download=True)\n",
    "\n",
    "# Define some example texts\n",
    "texts = [\n",
    "    \"I love this movie, it's fantastic!\",\n",
    "    \"This book is boring, I didn't enjoy it.\",\n",
    "    \"The food at that restaurant was delicious.\"\n",
    "]\n",
    "\n",
    "# Perform sentiment analysis on each text\n",
    "for text in texts:\n",
    "    predictions = model([text])\n",
    "    sentiment = predictions[0]  # Assuming sentiment is the first element of predictions\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85dbb4b-5e88-480f-b4fb-5b7189bc61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "\n",
    "# Load the pre-trained DeepPavlov model for sentence embeddings\n",
    "model = build_model(\"universal_sentence_encoder\", download=True)\n",
    "\n",
    "# Function to preprocess the textbook data\n",
    "def preprocess_textbook(textbook_path):\n",
    "    with open(textbook_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Function to find the most similar response\n",
    "def find_most_similar_response(user_input, responses):\n",
    "    user_embedding = model([user_input])\n",
    "    response_embeddings = model(responses)\n",
    "    similarities = [(i, cosine_similarity(user_embedding, resp_emb)) for i, resp_emb in enumerate(response_embeddings)]\n",
    "    max_similarity_idx = max(similarities, key=lambda x: x[1])[0]\n",
    "    return responses[max_similarity_idx]\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return sum(a * b for a, b in zip(embedding1, embedding2)) / (sum(a ** 2 for a in embedding1) ** 0.5 * sum(b ** 2 for b in embedding2) ** 0.5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    textbook_path = \"corpus_data/movie.txt\"  # Path to your textbook file\n",
    "    if not os.path.exists(textbook_path):\n",
    "        print(\"Textbook file not found. Please provide the correct path.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Preprocess the textbook data\n",
    "    textbook_responses = preprocess_textbook(textbook_path)\n",
    "\n",
    "    print(\"Welcome to the Textbook Chatbot! Type 'quit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"quit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Find the most similar response\n",
    "        response = find_most_similar_response(user_input, textbook_responses)\n",
    "        print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f853ad-a1b4-481c-8d77-00bec67f82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Define some example texts\n",
    "texts = [\n",
    "    \"I love this movie, it's fantastic!\",\n",
    "    \"This book is boring, I didn't enjoy it.\",\n",
    "    \"The food at that restaurant was delicious.\"\n",
    "]\n",
    "\n",
    "# Perform sentiment analysis on each text\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = logits.argmax().item()\n",
    "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c0e53-ff05-4ddc-9375-9c391058dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "\n",
    "# Load a pre-trained retrieval-based model\n",
    "model_config = configs.seq2seq.seq2seq_encoder_decoder  # This is an example, adjust based on your findings\n",
    "\n",
    "model = build_model(model_config, download=True)\n",
    "\n",
    "# Define greetings and small talk responses\n",
    "greetings = [\"hello\", \"hi\", \"hey there\"]\n",
    "small_talk = {\n",
    "    \"how are you\": [\"I'm doing well, thanks for asking! How about you?\"],\n",
    "    \"what's up\": [\"Not much, just hanging out. What are you up to?\"],\n",
    "    \"good morning\": [\"Good morning! ☀️\"],\n",
    "    \"good evening\": [\"Good evening! \"],\n",
    "}\n",
    "\n",
    "def chat_with_bot(user_input):\n",
    "  \"\"\"Handles user input and generates responses.\"\"\"\n",
    "  if user_input.lower() in greetings:\n",
    "    return f\"Hi!  Nice to hear from you.\"\n",
    "  elif user_input.lower() in small_talk:\n",
    "    return random.choice(small_talk[user_input.lower()])\n",
    "  else:\n",
    "    # Use the model for retrieval-based response generation\n",
    "    bot_response = model([user_input])[0]\n",
    "    return bot_response  # Replace with retrieved text for more complex responses\n",
    "\n",
    "# Main loop for user interaction\n",
    "while True:\n",
    "  user_input = input(\"You: \")\n",
    "  if user_input.lower() in ['exit', 'quit']:\n",
    "    print(\"Goodbye!\")\n",
    "    break\n",
    "  bot_response = chat_with_bot(user_input)\n",
    "  print(\"Bot:\", bot_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe92e57-b37b-4ce7-acd6-6266f3430347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "\n",
    "\n",
    "model = GPT4All(model_name='orca-mini-3b-gguf2-q4_0.gguf')\n",
    "with model.chat_session():\n",
    "    response1 = model.generate(prompt='hello', temp=0)\n",
    "    response2 = model.generate(prompt='write me a short poem', temp=0)\n",
    "    response3 = model.generate(prompt='thank you', temp=0)\n",
    "    print(model.current_chat_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49724f-3f05-4b3b-9d11-9e8814a8481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "tokens = []\n",
    "with model.chat_session():\n",
    "    for token in model.generate(\"What is the capital of France?\", streaming=True):\n",
    "        tokens.append(token)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fe182-4f86-4fe7-9138-a2c08193cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.chat_session():\n",
    "    resp = model.generate(\"Write a select Query\")\n",
    "print(resp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3ed79-9b8d-478c-a21c-27da2c84877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model.generate(\"Write a select Query\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773287b6-505a-428c-aed9-73d2d15ab61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Download and load the GPT-4All model (might take some time)\n",
    "model_name = \"EleutherAI/gpt-j-6B\"  # Adjust model name based on your setup\n",
    "generator = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "def chat():\n",
    "  \"\"\"\n",
    "  Interactive chat loop with the GPT-4All model.\n",
    "  \"\"\"\n",
    "  while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "      break\n",
    "    \n",
    "    # Generate response using GPT-4All\n",
    "    response = generator(user_input, max_length=100, do_sample=True, top_k=50, top_p=0.9)  # Adjust parameters as needed\n",
    "    print(\"Bot:\", response[0]['generated_text'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  print(\"Welcome to the GPT-4All Chatbot!\")\n",
    "  chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b30f9-c6de-4661-82c6-ee86a1a5c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "model_name = \"orca-mini-3b-gguf2-q4_0.gguf\"\n",
    "model = GPT4All(model_name=model_name)\n",
    "\n",
    "def chat():\n",
    "    \"\"\"\n",
    "    Interactive chat loop with the GPT4All model.\n",
    "    \"\"\"\n",
    "    print(\"Welcome to the GPT4All Chatbot!\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"quit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Generate response using GPT4All\n",
    "        response = model.generate(prompt=user_input, temp=0)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9514a8-2d10-4783-b668-907a24bd0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All('wizardlm-13b-v1.2.Q4_0.gguf')\n",
    "system_template = 'A chat between a curious user and an artificial intelligence assistant.\\n'\n",
    "# many models use triple hash '###' for keywords, Vicunas are simpler:\n",
    "prompt_template = 'USER: {0}\\nASSISTANT: '\n",
    "with model.chat_session(system_template, prompt_template):\n",
    "    response1 = model.generate('why is the grass green?')\n",
    "    print(response1)\n",
    "    print()\n",
    "    response2 = model.generate('why is the sky blue?')\n",
    "    print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c90eaf-e1cd-435e-af5f-1bb00e91b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chat.util import Chat, reflections\n",
    "\n",
    "# Define pairs of patterns and responses for the chatbot\n",
    "pairs = [\n",
    "    [\n",
    "        r\"my name is (.*)\",\n",
    "        [\"Hello %1, how can I help you today?\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"what is your name?\",\n",
    "        [\"My name is ChatBot and I'm here to assist you.\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"how are you ?\",\n",
    "        [\"I'm doing well, thank you!\", \"I'm fine, thanks!\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"quit\",\n",
    "        [\"Bye, take care. See you soon!\", \"Goodbye! Have a great day!\",]\n",
    "    ],\n",
    "]\n",
    "\n",
    "# Create a Chat instance with the defined pairs\n",
    "chatbot = Chat(pairs, reflections)\n",
    "\n",
    "def chat():\n",
    "    \"\"\"\n",
    "    Interactive chat loop with the NLTK chatbot.\n",
    "    \"\"\"\n",
    "    print(\"Welcome! Type 'quit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        response = chatbot.respond(user_input)\n",
    "        print(\"Bot:\", response)\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b59a6f-4652-47c9-a90f-5e8d9f60cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the .txt files\n",
    "directory = 'corpus_data/'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter out only the .txt files\n",
    "txt_files = [file for file in files if file.endswith('.txt')]\n",
    "\n",
    "# Loop through each .txt file\n",
    "for txt_file in txt_files:\n",
    "    # Construct the full path to the file\n",
    "    file_path = os.path.join(directory, txt_file)\n",
    "    \n",
    "    # Open the file and read its contents\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    \n",
    "    # Process the data as needed\n",
    "    print(f\"Contents of {txt_file}:\")\n",
    "    print(data)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9db472-7569-4b3e-a6af-6509a95516a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chat.util import Chat, reflections\n",
    "\n",
    "# Sample text data (you can replace this with your own text dataset)\n",
    "text_data = data\n",
    "\n",
    "# Preprocess the text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentences = nltk.sent_tokenize(text_data)\n",
    "word_tokens = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "filtered_sentences = [[word for word in words if word.isalnum() and word not in stop_words] for words in word_tokens]\n",
    "\n",
    "# Create a dictionary where each word is a key and its value is a list of sentences containing that word\n",
    "word_to_sentences = {}\n",
    "for sentence in filtered_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_sentences:\n",
    "            word_to_sentences[word] = []\n",
    "        word_to_sentences[word].append(sentence)\n",
    "\n",
    "# Define a function to find the most similar sentence based on word overlap\n",
    "def respond(input_sentence):\n",
    "    words = [word.lower() for word in word_tokenize(input_sentence) if word.isalnum() and word not in stop_words]\n",
    "    response_sentences = []\n",
    "    for word in words:\n",
    "        if word in word_to_sentences:\n",
    "            response_sentences.extend(word_to_sentences[word])\n",
    "    if response_sentences:\n",
    "        return ' '.join(random.choice(response_sentences))\n",
    "    else:\n",
    "        return \"I'm sorry, I don't understand.\"\n",
    "\n",
    "# Example interaction loop\n",
    "print(\"Welcome! Type 'quit' to end the conversation.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = respond(user_input)\n",
    "        print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd554f83-fd04-48c6-b18e-bb1e037a56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "# Load English tokenizer, tagger, parser, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read and preprocess text from the textbook\n",
    "with open(\"corpus_data/movie.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Train the SpaCy model on the preprocessed text data\n",
    "# Increase the maximum length to accommodate larger text\n",
    "nlp.max_length = len(text) + 1000000  # Adding extra buffer\n",
    "doc = nlp(text)\n",
    "\n",
    "# Function to generate a response based on user input\n",
    "def generate_response(user_input):\n",
    "    user_doc = nlp(user_input)\n",
    "    # Here you can implement logic to process the user input and generate a response\n",
    "    # For simplicity, let's just select a random sentence from the textbook\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return random.choice(sentences)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the Textbook Chatbot! Type 'quit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"quit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        response = generate_response(user_input)\n",
    "        print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85250bf1-0848-4061-bc1d-c0595ad69a0b",
   "metadata": {},
   "source": [
    "To create a chatbot trained on top of your textbook, you can use various machine learning approaches, including both retrieval-based and generative models. Here are some options:\n",
    "\n",
    "1. **Retrieval-Based Models**:\n",
    "    - **TF-IDF Vectorization**: You can represent each document (e.g., sentence or paragraph) in your textbook as a TF-IDF vector. When a user inputs a query, you can calculate the similarity between the query and each document vector to retrieve the most similar document as the response.\n",
    "    - **Word Embeddings + Cosine Similarity**: Instead of TF-IDF, you can use word embeddings (e.g., Word2Vec, GloVe) to represent documents and queries as dense vectors. Then, calculate the cosine similarity between the query and each document vector to retrieve the most similar document.\n",
    "    - **Pre-trained Language Models**: Use pre-trained language models like BERT or Universal Sentence Encoder to encode the input query and documents, then calculate similarity scores to retrieve the most relevant document.\n",
    "\n",
    "2. **Generative Models**:\n",
    "    - **Seq2Seq Models**: Train a sequence-to-sequence model (e.g., using LSTM or Transformer architecture) on pairs of input-output sequences. Each input can be a user query, and the corresponding output is the response from your textbook.\n",
    "    - **GPT-Based Models**: Fine-tune a GPT model (e.g., GPT-3, GPT-4) on your textbook data. Although GPT models are primarily used for generating text, you can fine-tune them on a dataset to learn the specific style and content of your textbook.\n",
    "\n",
    "3. **Hybrid Approaches**:\n",
    "    - **Retrieval + Generative**: Combine retrieval-based and generative approaches. Use a retrieval-based model to retrieve a set of candidate responses based on the user query, then use a generative model to select or refine the final response from the candidates.\n",
    "    - **Rule-Based Filtering**: Pre-process the user query to identify specific topics or intents and filter the search space to only relevant sections of your textbook. Then, apply a retrieval or generative model to generate responses from the filtered content.\n",
    "\n",
    "Each approach has its advantages and limitations, and the choice depends on factors like the size of your dataset, computational resources, desired level of customization, and the specific requirements of your chatbot.\n",
    "\n",
    "\n",
    "**LSTM (Long Short-Term Memory)**:\n",
    "LSTM model is a type of recurrent neural network (RNN) architecture designed to model sequential data and overcome the vanishing gradient problem that traditional RNNs face. \r\n",
    "\r\n",
    "LSTM networks are capable of learning long-term dependencies in data by maintaining a cell state, which can be updated and modified through carefully designed gates. These gates regulate the flow of information, allowing LSTM networks to capture dependencies over long time lags and hence make them effective for tasks like natural language processing, time series prediction, and speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "008868e5-54b1-49ef-a2b1-813e86945695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\pedam\\AppData\\Local\\Temp\\__autograph_generated_filec2idwjx4.py\", line 11, in tf__call\n        (_, encoder_state_h, encoder_state_c) = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(encoder_inputs),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"seq2_seq_model_3\" \"                 f\"(type Seq2SeqModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\pedam\\AppData\\Local\\Temp\\ipykernel_20488\\3889586601.py\", line 30, in call  *\n            _, encoder_state_h, encoder_state_c = self.encoder(encoder_inputs)\n        File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py\", line 553, in __call__  **\n            return super().__call__(inputs, **kwargs)\n        File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 232, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"lstm_6\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 3)\n    \n    \n    Call arguments received by layer \"seq2_seq_model_3\" \"                 f\"(type Seq2SeqModel):\n      • inputs=('tf.Tensor(shape=(None, 3), dtype=int32)', 'tf.Tensor(shape=(None, 2), dtype=int32)')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefs7k8ket.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filec2idwjx4.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m (encoder_inputs, decoder_inputs) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(inputs)\n\u001b[1;32m---> 11\u001b[0m (_, encoder_state_h, encoder_state_c) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mencoder, (ag__\u001b[38;5;241m.\u001b[39mld(encoder_inputs),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m encoder_states \u001b[38;5;241m=\u001b[39m [ag__\u001b[38;5;241m.\u001b[39mld(encoder_state_h), ag__\u001b[38;5;241m.\u001b[39mld(encoder_state_c)]\n\u001b[0;32m     13\u001b[0m (decoder_outputs, _, _) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdecoder, (ag__\u001b[38;5;241m.\u001b[39mld(decoder_inputs),), \u001b[38;5;28mdict\u001b[39m(initial_state\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(encoder_states)), fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\pedam\\AppData\\Local\\Temp\\__autograph_generated_filec2idwjx4.py\", line 11, in tf__call\n        (_, encoder_state_h, encoder_state_c) = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(encoder_inputs),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"seq2_seq_model_3\" \"                 f\"(type Seq2SeqModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\pedam\\AppData\\Local\\Temp\\ipykernel_20488\\3889586601.py\", line 30, in call  *\n            _, encoder_state_h, encoder_state_c = self.encoder(encoder_inputs)\n        File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py\", line 553, in __call__  **\n            return super().__call__(inputs, **kwargs)\n        File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\ProgramData\\anaconda3\\envs\\deeppavlov_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 232, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"lstm_6\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 3)\n    \n    \n    Call arguments received by layer \"seq2_seq_model_3\" \"                 f\"(type Seq2SeqModel):\n      • inputs=('tf.Tensor(shape=(None, 3), dtype=int32)', 'tf.Tensor(shape=(None, 2), dtype=int32)')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sample training data\n",
    "input_data = ['hello', 'how are you', 'goodbye']\n",
    "output_data = ['hi', 'I am fine', 'bye']\n",
    "\n",
    "# Tokenization\n",
    "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "output_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "input_tokenizer.fit_on_texts(input_data)\n",
    "output_tokenizer.fit_on_texts(output_data)\n",
    "\n",
    "input_seq = input_tokenizer.texts_to_sequences(input_data)\n",
    "output_seq = output_tokenizer.texts_to_sequences(output_data)\n",
    "\n",
    "input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq)\n",
    "output_seq = tf.keras.preprocessing.sequence.pad_sequences(output_seq)\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class Seq2SeqModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.encoder = tf.keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "        self.decoder = tf.keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "        _, encoder_state_h, encoder_state_c = self.encoder(encoder_inputs)\n",
    "        encoder_states = [encoder_state_h, encoder_state_c]\n",
    "        decoder_outputs, _, _ = self.decoder(decoder_inputs, initial_state=encoder_states)\n",
    "        logits = self.dense(decoder_outputs)\n",
    "        return logits\n",
    "\n",
    "# Initialize and compile the model\n",
    "vocab_size = len(input_tokenizer.word_index) + 1\n",
    "embedding_dim = 128\n",
    "hidden_units = 256\n",
    "\n",
    "model = Seq2SeqModel(vocab_size, embedding_dim, hidden_units)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([input_seq, output_seq[:, :-1]], output_seq[:, 1:], batch_size=2, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b448a31d-df25-4d5d-898f-a2a460f6daa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 3s 28ms/step - loss: 2.4917 - sparse_categorical_accuracy: 0.1667\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 2.0393 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.2911 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0076 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.1951 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9060 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.9196 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.1122 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.0833 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 1.0705 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.9409 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.9225 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7626 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7926 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7270 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7016 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.8799 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7015 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.9831 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6798 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8877 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7118 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6480 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6759 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7517 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6973 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.8185 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7459 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6088 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6578 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7170 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7177 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.9002 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6150 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6366 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6011 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.8151 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8097 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7115 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7068 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7890 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.9494 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.9290 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7707 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7523 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.8073 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8040 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.6473 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7811 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8300 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7564 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7437 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.8472 - sparse_categorical_accuracy: 0.1667\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8257 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.8394 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7402 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7825 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6834 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.8199 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7908 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7267 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6273 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7416 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7844 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7429 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6916 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7760 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7679 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.7457 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.8126 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6892 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7676 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6971 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7071 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.7231 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6509 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7165 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7008 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6637 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7685 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.8757 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.9871 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7638 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6881 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6882 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7205 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7846 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7789 - sparse_categorical_accuracy: 0.1667\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7364 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7966 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6640 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8124 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6562 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7634 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6653 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7768 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7680 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7615 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.7230 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6968 - sparse_categorical_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17e8b2b0b20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Masking, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample training data\n",
    "input_data = ['hello', 'how are you', 'goodbye']\n",
    "output_data = ['hi', 'I am fine', 'bye']\n",
    "\n",
    "# Tokenization\n",
    "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "output_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "input_tokenizer.fit_on_texts(input_data)\n",
    "output_tokenizer.fit_on_texts(output_data)\n",
    "\n",
    "input_seq = input_tokenizer.texts_to_sequences(input_data)\n",
    "output_seq = output_tokenizer.texts_to_sequences(output_data)\n",
    "\n",
    "input_seq = pad_sequences(input_seq)\n",
    "output_seq = pad_sequences(output_seq)\n",
    "\n",
    "# Define the Transformer model\n",
    "def transformer_model(vocab_size, d_model, num_heads, num_layers, dropout_rate, name=\"transformer\"):\n",
    "    inputs = Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # Embedding layers\n",
    "    input_embedding = Embedding(vocab_size, d_model)(inputs)\n",
    "    dec_embedding = Embedding(vocab_size, d_model)(dec_inputs)\n",
    "\n",
    "    # Masking layers\n",
    "    input_masking = Masking(mask_value=0)(input_embedding)\n",
    "    dec_masking = Masking(mask_value=0)(dec_embedding)\n",
    "\n",
    "    # Encoder\n",
    "    enc_outputs = input_masking\n",
    "    for i in range(num_layers):\n",
    "        enc_outputs = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)(enc_outputs, enc_outputs)\n",
    "        enc_outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(enc_outputs)\n",
    "\n",
    "    # Decoder\n",
    "    dec_outputs = dec_masking\n",
    "    for i in range(num_layers):\n",
    "        dec_outputs = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)(dec_outputs, enc_outputs)\n",
    "        dec_outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(dec_outputs)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(vocab_size)(dec_outputs)\n",
    "\n",
    "    return Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "\n",
    "# Initialize and compile the model\n",
    "vocab_size = len(input_tokenizer.word_index) + 1\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "model = transformer_model(vocab_size, d_model, num_heads, num_layers, dropout_rate)\n",
    "model.compile(optimizer=Adam(1e-4), loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[SparseCategoricalAccuracy()])\n",
    "\n",
    "# Train the model\n",
    "model.fit([input_seq, output_seq[:, :-1]], output_seq[:, 1:], batch_size=2, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fac4253e-6131-4fcb-9d87-7767d873343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 701ms/step\n",
      "Bot: \n"
     ]
    }
   ],
   "source": [
    "def preprocess_input(text, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text.\n",
    "    \"\"\"\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    return padded_sequence\n",
    "\n",
    "def generate_response(input_text, model, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Generates a response to the input text using the trained model.\n",
    "    \"\"\"\n",
    "    input_sequence = preprocess_input(input_text, tokenizer, max_length)\n",
    "    output_sequence = np.array([[0]])  # Start token\n",
    "    for _ in range(max_length):\n",
    "        predictions = model.predict([input_sequence, output_sequence])\n",
    "        predicted_id = np.argmax(predictions[:, -1, :])\n",
    "        if predicted_id == 0:  # End token\n",
    "            break\n",
    "        output_sequence = np.append(output_sequence, [[predicted_id]], axis=-1)\n",
    "    return output_sequence\n",
    "\n",
    "def postprocess_output(output_sequence, tokenizer):\n",
    "    \"\"\"\n",
    "    Postprocesses the model's output sequence into human-readable text.\n",
    "    \"\"\"\n",
    "    decoded_text = tokenizer.sequences_to_texts(output_sequence)[0]\n",
    "    return decoded_text.strip()\n",
    "\n",
    "# Example usage\n",
    "input_text = \"hello\"\n",
    "max_length = 10000\n",
    "response_sequence = generate_response(input_text, model, output_tokenizer, max_length)\n",
    "response_text = postprocess_output(response_sequence, output_tokenizer)\n",
    "print(\"Bot:\", response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760b892-a613-4a81-b0a1-6b9672290df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeppavlov_env",
   "language": "python",
   "name": "deeppavlov_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
